{
  "search": "Suche",
  "loading": "Laden",
  "noData": "Keine Daten",
  "cancel": "Abbrechen",
  "save": "Speichern",
  "saveSubmit": "Speichern & Absenden",
  "submit": "Absenden",
  "sureQ": "Bist du sicher?",
  "clearConversations": "Alle Konversationen löschen",
  "newSystemPrompt": "Neue Systemaufforderung",
  "name": "Name",
  "prompt": "Prompt",
  "systemPrompt": "Systemaufforderung",
  "systemPromptDescription": "The system prompt to use when sending a message",
  "model": "Modell",
  "modelDescription": "The model used for this conversation",
  "models": "Modelle",
  "chat": {
    "newConversation": "Neue Konversation",
    "newFolder": "Neuer Ordner",
    "noData": "Keine Daten",
    "enterMessage": "Bitte gib eine Nachricht ein",
    "messageLimit": "Das Nachrichtenlimit beträgt {{maxLength}} Zeichen. Du hast bereits {{valueLength}} Zeichen eingegeben.",
    "stopGenerating": "Generieren stoppen",
    "regenerateResponse": "Antwort erneut generieren",
    "startTyping": "Start typing, type / to select a template...",
    "error": "Entschuldigung, es ist ein Fehler aufgetreten.",
    "repeatPenalty": "Repeat Penalty",
    "repeatPenaltyDescription": "Positive values penalize new tokens based on their existing frequency in the text so far. Decreases the model's likelihood to repeat the same line verbatim. Also known as 'frequency penalty'. Defaults to model provider configuration.",
    "presencePenalty": "Repeat Penalty",
    "presencePenaltyDescription": "Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics. Defaults to model provider configuration.",
    "maxTokens": "Max Tokens",
    "maxTokensDescription": "The maximum number of tokens to generate. The higher the number, the longer the AI will take to generate a response.",
    "seed": "Seed",
    "seedDescription": "The seed used to generate the response. The same seed will always generate the same response.",
    "stop": "Stop",
    "stopDescription": "Comma-separated list of tokens to stop generation on. The AI will stop generating tokens once it encounters any of these tokens.",
    "temperature": "Temperature",
    "temperatureDescription": "Higher values will make the output more random, while lower values will make it more focused and deterministic.",
    "precise": "Precise",
    "neutral": "Neutral",
    "creative": "Creative",
    "topK": "Top K",
    "topKDescription": "The number of highest probability vocabulary tokens to keep for top-k-filtering. Between 1 and infinity. Defaults to model provider configuration.",
    "topP": "Top P",
    "topPDescription": "The cumulative probability of parameter highest probability tokens to use for nucleus sampling, between 0 and 1. Defaults to model provider configuration."
  },
  "markdown": {
    "enterFileName": "Dateinamen eingeben",
    "copied": "Kopiert!",
    "copyCode": "Code kopieren"
  },
  "promptbar": {
    "newFolder": "Neuer Ordner",
    "newMessageTemplate": "Neue Nachrichten Vorlage",
    "name": "Name",
    "description": "Beschreibung",
    "prompt": "Prompt",
    "promptContent": "Prompt content. Use {{}} to denote a variable. Ex: {{name}} is a {{adjective}} {{noun}}",
    "save": "Speichern"
  },
  "sidebar": {
    "importData": "Import Data",
    "exportData": "Export Data"
  },
  "error": {
    "fetchingModels": "Fehler beim Abrufen der Sprachmodelle.",
    "keyMissing": "Stelle sicher, dass dein {{vendor}} Schlüssel in der unteren linken Ecke der Seitenleiste eingetragen ist.",
    "vendorIssue": "Wenn dies der Fall ist, könnte {{vendor}} möglicherweise momentan Probleme haben."
  }
}
